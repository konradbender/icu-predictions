{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did provide the file ``` requirements.txt ``` though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import data\n",
    "import preprocessing\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "import prepare_submission\n",
    "from sklearn.model_selection import train_test_split\n",
    "import importlib\n",
    "import copy\n",
    "from sklearn.model_selection import KFold\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Pre Processing\n",
    "\n",
    "As described above, the train and test features contain a lot of missing values. Therefore, we need to develop a\n",
    "strategy to deal with these missing values. In a nutshell, we are imputing with the median value for each feature\n",
    "and standardizing each feature to zero mean and unit variance. This is happening in a number of steps\n",
    "\n",
    "1. ```preprocessing.prepare_features()``` Fills up values for each for each patient based on their data. This means\n",
    "that if patient ```i```has a missing feature at a certain timestamp, it will be filled up with the median from the other\n",
    "timestamps. Important: If a patient is missing a value for a feature fora all timestamps, those values are left as\n",
    " missing and will be filled up in a next step. Finally, each patient is flattened into a single row vector.\n",
    "\n",
    "2. ```preprocessing.impute_features()``` Looks at the flattened vectors of patients in the train and test dataset. Here,\n",
    "missing values are again imputed based on the median, so if patient ```i```was missing values for blood pressure at all\n",
    "timestamps, then those are filled up here with the median blood pressure of all patients for each timestamp ```j```.\n",
    "After calling this method, the train and test features will not contain any ```np.nan```anymore.\n",
    "\n",
    "3. ```preprocessing.standardize_features()``` standardizes the features to unit variance and zero mean over train and\n",
    "test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "split = False\n",
    "\n",
    "raw_train_features = data.get_training_features()\n",
    "raw_train_labels = data.get_training_labels()\n",
    "raw_test_features = data.get_test_features()\n",
    "\n",
    "if split==False:\n",
    "    #load data from files\n",
    "    #reshape the features in order to have one row per patient and impute on a per-patient-level\n",
    "    importlib.reload(preprocessing)\n",
    "    reshaped_train_features = preprocessing.prepare_features(raw_train_features,\n",
    "                                                            appendix=\"train\", read_from_file=False)\n",
    "    reshaped_test_features = preprocessing.prepare_features(raw_test_features, appendix='test')\n",
    "\n",
    "    #Fill out the missing data points\n",
    "    train, test = preprocessing.impute_features(reshaped_train_features,reshaped_test_features)\n",
    "\n",
    "    #Standardize the features\n",
    "    train_features, test_features = preprocessing.standardize_features(train, test)\n",
    "    train_labels = raw_train_labels\n",
    "\n",
    "else:\n",
    "    #splitting strategy in order to be able to give a score to our models\n",
    "    del raw_test_features # we dont need them anymore so lets free memory\n",
    "\n",
    "    reshaped_train_features = preprocessing.prepare_features(raw_train_features, read_from_file=False)\n",
    "\n",
    "    # make a split\n",
    "    s_train_features, s_test_features, s_train_labels, s_test_labels = train_test_split(\n",
    "        copy.deepcopy(reshaped_train_features), copy.deepcopy(raw_train_labels),  test_size = 0.33)\n",
    "\n",
    "\n",
    "    # fill out values that were not imputed in last step becasue a patient was missing all of them\n",
    "    train, test = preprocessing.impute_features(s_train_features,s_test_features)\n",
    "\n",
    "    train_features, test_features = preprocessing.standardize_features(train, test)\n",
    "    train_labels = s_train_labels\n",
    "    test_labels = s_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare submission instance\n",
    "our_submission = prepare_submission.Submission(test_features.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1\n",
    "\n",
    "For this model, we use GradientBoostingClassifier, which fits a number of decision trees on the data. Trees are great\n",
    "for this application because they offer non-linearity without us having to use a neural network.\n",
    "When researching the best methods for fitting trees to our data, we read that GBCs offer a very good model\n",
    "because additional trees are fitted to predict the negative gradient of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "relevant_labels_1 = ['LABEL_BaseExcess','LABEL_Fibrinogen','LABEL_AST','LABEL_Alkalinephos',\n",
    " 'LABEL_Bilirubin_total','LABEL_Lactate','LABEL_TroponinI','LABEL_SaO2',\n",
    " 'LABEL_Bilirubin_direct','LABEL_EtCO2']\n",
    "\n",
    "models = {label:None for label in relevant_labels}\n",
    "probabilities_1 = copy.deepcopy(models)\n",
    "threads = copy.deepcopy(models)\n",
    "\n",
    "model_params = {'loss': 'deviance', 'random_state': 0}\n",
    "\n",
    "# Here we fit all the models\n",
    "for label, model in models.items():\n",
    "    models[label] = GradientBoostingClassifier(**model_params)\n",
    "    threads[label] = threading.Thread(target=models[label].fit, args=[train_features,train_labels[label]])\n",
    "    threads[label].start()\n",
    "\n",
    "for label, thread in threads.items():\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the predictions on the test set\n",
    "for label, model in models.items():\n",
    "    prob_vector = model.predict_proba(test_features)[:,1]\n",
    "    probabilities_1[label] = prob_vector\n",
    "\n",
    "# Store predictions in a separate csv file\n",
    "importlib.reload(prepare_submission)\n",
    "our_submission.add_task_1_dict(probabilities_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Subtask 2\n",
    "\n",
    "Here we use the same strategy as in task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_2 = 'LABEL_Sepsis'\n",
    "model_params = {'loss': 'deviance',  'random_state': 0}\n",
    "\n",
    "model =  GradientBoostingClassifier(**model_params, n_estimators=200)\n",
    "model.fit(train_features,train_labels[labe_2l])\n",
    "\n",
    "#compute predicitons on the test set\n",
    "probabilities_2 = model.predict_proba(test_features)\n",
    "\n",
    "#store predictions in a separate file\n",
    "importlib.reload(prepare_submission)\n",
    "our_submission.add_task_2(probabilities_2[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Subtask 3\n",
    "\n",
    "The idea here is to fit a linear regressor with a ridge penalty determined with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "relevant_labels_3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
    "train_labels = train_labels[relevant_labels_3]\n",
    "\n",
    "splitter = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "#Build model\n",
    "def scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return r2_score(y,y_pred)\n",
    "\n",
    "general_parameters = {'fit_intercept':True, 'scoring':scorer, 'cv': splitter}\n",
    "models = {key:RidgeCV(**general_parameters) for key in relevant_labels}\n",
    "\n",
    "#Fit model\n",
    "for index,label in enumerate(relevant_labels_3):\n",
    "    models[label].fit(train_features,train_labels[label])\n",
    "\n",
    "#Make predictions\n",
    "submission_predictions = {label: None for label in relevant_labels}\n",
    "for label, model in models.items():\n",
    "    submission_predictions[label] = model.predict(test_features)\n",
    "\n",
    "#Store prediction\n",
    "our_submission.add_task_3(submission_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### How do the models perform?\n",
    "Now if we set the variable split to True in the beginning of the notebook, we can split the training data into a train set and a validation set. This way we can see how our models perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if split:\n",
    "    importlib.reload(score_submission)\n",
    "    score_submission.get_score(test_labels,our_submission.data)\n",
    "else:\n",
    "    our_submission.save()\n",
    "    our_submission.data.to_csv('final_submission.csv', index = True, float_format='%.3f')\n",
    "    our_submission.data.to_csv('final_submission.zip', index = True, float_format='%.3f', compression='zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
